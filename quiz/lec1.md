Question 1

Q: How does the relationship between FLOPs spent in attention layers versus MLP layers change with model scale?
A. Attention layers consistently dominate FLOPs usage regardless of model size.
B. The ratio of FLOPs between attention and MLP layers remains relatively constant across scales.
C. At small scales, FLOPs are comparable, but MLP layers dominate at very large scales.
D. At large scales, optimizing attention layers becomes the most critical factor for efficiency.
Answer: C
Time: 5:21–5:52

Question 2

Q: What does the phenomenon of "emergent behavior" in large language models imply?
A. Models develop consciousness as they scale up.
B. Certain abilities, like in-context learning, only appear after a model reaches a significant size.
C. Smaller models can be fine-tuned to exhibit the same behaviors as larger ones.
D. The model's performance grows linearly with the amount of training FLOPs.
Answer: B
Time: 6:20–6:50

Question 3

Q: What is the correct interpretation of the "bitter lesson" as presented in the lecture?
A. Scale is the only thing that matters, and algorithms are irrelevant.
B. Algorithms that perform well at scale are what truly matter for advancing AI.
C. Human-like cognitive architectures will eventually outperform scaling.
D. Any gains from better algorithms are quickly nullified by increased compute.
Answer: B
Time: 9:37–9:52


Question 4

Q: What makes the autoregressive decoding part of inference particularly difficult to optimize?
A. It requires a massive amount of VRAM.
B. It is computationally intensive due to large matrix multiplications.
C. It is often memory-bound because tokens are generated one at a time, making it hard to saturate the GPU.
D. It can only be performed on CPUs, not GPUs.
Answer: C
Time: 38:58–39:10

Question 5

Q: What is speculative decoding?
A. A technique to guess the user's next prompt.
B. A method where a smaller, cheaper model generates multiple tokens that are then verified by the main model.
C. A way to train models on hypothetical or "speculative" data.
D. An algorithm for finding the single most likely next token.
Answer: B
Time: 39:22–39:34

Question 6

Q: What is a primary finding from looking at raw Common Crawl data?
A. It is a highly curated and clean dataset, ideal for training.
B. It consists mostly of academic papers and books.
C. A large portion of the data is "trash" or spammy, requiring significant filtering.
D. It lacks data in languages other than English.
Answer: C
Time: 48:05–48:15

Question 7

Q: Why is deduplication of the training data an important step?
A. To reduce the total size of the dataset and save storage costs.
B. To prevent the model from overfitting and memorizing specific examples.
C. To comply with copyright laws regarding data usage.
D. To ensure the model is trained on a diverse range of topics.
Answer: B
Time: 49:55-50:00

Question 8

Q: What is the goal of the "Alignment" phase of model training?
A. To increase the model's raw intelligence and knowledge.
B. To make the model's architecture more efficient.
C. To make the model useful by teaching it to follow instructions, adhere to a specific style, and be safe.
D. To compress the model's weights for faster deployment.
Answer: C
Time: 50:43–50:50

Question 9

Q: What kind of data is used in Supervised Fine-Tuning (SFT)?
A. A large corpus of unlabeled web text.
B. Pairs of prompts and desired responses.
C. Preference data indicating which of two responses is better.
D. Data generated by the model itself.
Answer: B
Time: 51:49–51:59

Question 10

Q: What is the core function of a tokenizer in a language model pipeline?
A. To translate text from one language to another.
B. To correct grammatical errors in the input string.
C. To convert raw text strings into sequences of integers (tokens) and back.
D. To filter out harmful or irrelevant content from the input.
Answer: C
Time: 1:00:49–1:01:12

Question 11

Q: What is a significant drawback of using simple character-based tokenization with Unicode code points?
A. It cannot handle emojis or special characters.
B. The sequence length becomes excessively long.
C. The vocabulary size becomes very large, and it's an inefficient use of the vocabulary for rare characters.
D. The encoding is not reversible, leading to data loss.
Answer: C
Time: 1:06:12–1:06:49

Question 12

Q: What is the main problem with pure byte-based tokenization?
A. The vocabulary size is too large (over 65,000).
B. It results in very long token sequences, which is inefficient for models with quadratic attention complexity.
C. It cannot represent all characters in the UTF-8 standard.
D. It is computationally expensive to convert strings to bytes.
Answer: B
Time: 1:08:20–1:08:47

Question 13

Q: What is a major issue with traditional word-based tokenization that BPE aims to solve?
A. It splits words into too many sub-word units.
B. It cannot handle punctuation or spaces.
C. It produces an unbounded vocabulary and struggles with rare or unseen words (out-of-vocabulary problem).
D. It is not adaptive to the statistics of the training corpus.
Answer: C
Time: 1:10:04–1:10:39


Question 14

Q: What is the fundamental principle of the BPE algorithm?
A. It splits words based on a fixed set of grammatical rules.
B. It learns to merge the most frequently occurring adjacent pair of tokens iteratively.
C. It assigns a unique token to every word found in the training corpus.
D. It breaks down all text into individual bytes.
Answer: B
Time: 1:12:49–1:12:54


Question 15

Q: When using a trained BPE tokenizer to encode a new string, what is the process?
A. The string is first broken into words, and each word is looked up in the vocabulary.
B. The string is converted to bytes, and then the learned merge operations are replayed in the same order they were learned.
C. The string is compared against the entire training corpus to find the closest match.
D. The BPE algorithm is re-run from scratch on the new string.
Answer: B
Time: 1:16:44–1:16:51

Question 16

Q: The lecture mentions a "compression ratio" for tokenizers. What does this ratio represent?
A. The number of tokens divided by the number of words.
B. The number of bytes in the original string divided by the number of tokens generated.
C. The size of the model vocabulary divided by the size of the training data.
D. The time it takes to encode versus decode a string.
Answer: B
Time: 1:04:40–1:04:48


Question 17

Q: The lecture mentions that if you have a lot of data but are compute-constrained, you might filter data aggressively. Why?
A. To reduce data storage costs.
B. To avoid legal issues with copyrighted data.
C. To avoid wasting precious compute on bad or irrelevant data.
D. To make the model smaller and faster for inference.
Answer: C
Time: 56:43–56:49


Question 18

Q: What distinguishes the "pre-fill" phase from the "decode" phase in inference?
A. Pre-fill is memory-bound, while decode is compute-bound.
B. Pre-fill processes the prompt tokens in parallel, while decode generates tokens one by one.
C. Pre-fill happens during training, while decode happens during deployment.
D. Pre-fill uses a small model, while decode uses a large model.
Answer: B
Time: 38:29–38:41


Question 19

Q: Why is having a good base model crucial before the alignment phase?
A. Alignment cannot add new knowledge; it can only modify a model that already has raw potential.
B. A good base model is smaller and cheaper to align.
C. The alignment algorithms like DPO only work on models trained on web-scale data.
D. The base model determines the final vocabulary size.
Answer: A
Time: 50:36–52:12

Question 20

Q: By convention, where is the space character typically placed by tokenizers like BPE?
A. It is discarded during tokenization.
B. It is appended to the end of a token.
C. It is converted into a special <SPACE> token.
D. It is prepended to the beginning of the following token.
Answer: D
Time: 1:02:19–1:02:19 
