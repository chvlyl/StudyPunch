[
  {
    "week": 1,
    "problem_numer": 1,
    "type": "multiple-choice",
    "question": "How does the relationship between FLOPs spent in attention layers versus MLP layers change with model scale?",
    "options": {
      "A": "Attention layers consistently dominate FLOPs usage regardless of model size.",
      "B": "The ratio of FLOPs between attention and MLP layers remains relatively constant across scales.",
      "C": "At small scales, FLOPs are comparable, but MLP layers dominate at very large scales.",
      "D": "At large scales, optimizing attention layers becomes the most critical factor for efficiency."
    },
    "answer": "C",
    "explaination": "time 5:21–5:52"
  },
  {
    "week": 1,
    "problem_numer": 2,
    "type": "multiple-choice",
    "question": "What does the phenomenon of \"emergent behavior\" in large language models imply?",
    "options": {
      "A": "Models develop consciousness as they scale up.",
      "B": "Certain abilities, like in-context learning, only appear after a model reaches a significant size.",
      "C": "Smaller models can be fine-tuned to exhibit the same behaviors as larger ones.",
      "D": "The model's performance grows linearly with the amount of training FLOPs."
    },
    "answer": "B",
    "explaination": "time 6:20–6:50"
  },
  {
    "week": 1,
    "problem_numer": 3,
    "type": "multiple-choice",
    "question": "What is the correct interpretation of the \"bitter lesson\" as presented in the lecture?",
    "options": {
      "A": "Scale is the only thing that matters, and algorithms are irrelevant.",
      "B": "Algorithms that perform well at scale are what truly matter for advancing AI.",
      "C": "Human-like cognitive architectures will eventually outperform scaling.",
      "D": "Any gains from better algorithms are quickly nullified by increased compute."
    },
    "answer": "B",
    "explaination": "time 9:37–9:52"
  },
  {
    "week": 1,
    "problem_numer": 4,
    "type": "multiple-choice",
    "question": "What makes the autoregressive decoding part of inference particularly difficult to optimize?",
    "options": {
      "A": "It requires a massive amount of VRAM.",
      "B": "It is computationally intensive due to large matrix multiplications.",
      "C": "It is often memory-bound because tokens are generated one at a time, making it hard to saturate the GPU.",
      "D": "It can only be performed on CPUs, not GPUs."
    },
    "answer": "C",
    "explaination": "time 38:58–39:10"
  },
  {
    "week": 1,
    "problem_numer": 5,
    "type": "multiple-choice",
    "question": "What is speculative decoding?",
    "options": {
      "A": "A technique to guess the user's next prompt.",
      "B": "A method where a smaller, cheaper model generates multiple tokens that are then verified by the main model.",
      "C": "A way to train models on hypothetical or \"speculative\" data.",
      "D": "An algorithm for finding the single most likely next token."
    },
    "answer": "B",
    "explaination": "time 39:22–39:34"
  },
  {
    "week": 1,
    "problem_numer": 6,
    "type": "multiple-choice",
    "question": "What is a primary finding from looking at raw Common Crawl data?",
    "options": {
      "A": "It is a highly curated and clean dataset, ideal for training.",
      "B": "It consists mostly of academic papers and books.",
      "C": "A large portion of the data is \"trash\" or spammy, requiring significant filtering.",
      "D": "It lacks data in languages other than English."
    },
    "answer": "C",
    "explaination": "time 48:05–48:15"
  },
  {
    "week": 1,
    "problem_numer": 7,
    "type": "multiple-choice",
    "question": "Why is deduplication of the training data an important step?",
    "options": {
      "A": "To reduce the total size of the dataset and save storage costs.",
      "B": "To prevent the model from overfitting and memorizing specific examples.",
      "C": "To comply with copyright laws regarding data usage.",
      "D": "To ensure the model is trained on a diverse range of topics."
    },
    "answer": "B",
    "explaination": "time 49:55-50:00"
  },
  {
    "week": 1,
    "problem_numer": 8,
    "type": "multiple-choice",
    "question": "What is the goal of the \"Alignment\" phase of model training?",
    "options": {
      "A": "To increase the model's raw intelligence and knowledge.",
      "B": "To make the model's architecture more efficient.",
      "C": "To make the model useful by teaching it to follow instructions, adhere to a specific style, and be safe.",
      "D": "To compress the model's weights for faster deployment."
    },
    "answer": "C",
    "explaination": "time 50:43–50:50"
  },
  {
    "week": 1,
    "problem_numer": 9,
    "type": "multiple-choice",
    "question": "What kind of data is used in Supervised Fine-Tuning (SFT)?",
    "options": {
      "A": "A large corpus of unlabeled web text.",
      "B": "Pairs of prompts and desired responses.",
      "C": "Preference data indicating which of two responses is better.",
      "D": "Data generated by the model itself."
    },
    "answer": "B",
    "explaination": "time 51:49–51:59"
  },
  {
    "week": 1,
    "problem_numer": 10,
    "type": "multiple-choice",
    "question": "What is the core function of a tokenizer in a language model pipeline?",
    "options": {
      "A": "To translate text from one language to another.",
      "B": "To correct grammatical errors in the input string.",
      "C": "To convert raw text strings into sequences of integers (tokens) and back.",
      "D": "To filter out harmful or irrelevant content from the input."
    },
    "answer": "C",
    "explaination": "time 1:00:49–1:01:12"
  },
  {
    "week": 1,
    "problem_numer": 11,
    "type": "multiple-choice",
    "question": "What is a significant drawback of using simple character-based tokenization with Unicode code points?",
    "options": {
      "A": "It cannot handle emojis or special characters.",
      "B": "The sequence length becomes excessively long.",
      "C": "The vocabulary size becomes very large, and it's an inefficient use of the vocabulary for rare characters.",
      "D": "The encoding is not reversible, leading to data loss."
    },
    "answer": "C",
    "explaination": "time 1:06:12–1:06:49"
  },
  {
    "week": 1,
    "problem_numer": 12,
    "type": "multiple-choice",
    "question": "What is the main problem with pure byte-based tokenization?",
    "options": {
      "A": "The vocabulary size is too large (over 65,000).",
      "B": "It results in very long token sequences, which is inefficient for models with quadratic attention complexity.",
      "C": "It cannot represent all characters in the UTF-8 standard.",
      "D": "It is computationally expensive to convert strings to bytes."
    },
    "answer": "B",
    "explaination": "time 1:08:20–1:08:47"
  },
  {
    "week": 1,
    "problem_numer": 13,
    "type": "multiple-choice",
    "question": "What is a major issue with traditional word-based tokenization that BPE aims to solve?",
    "options": {
      "A": "It splits words into too many sub-word units.",
      "B": "It cannot handle punctuation or spaces.",
      "C": "It produces an unbounded vocabulary and struggles with rare or unseen words (out-of-vocabulary problem).",
      "D": "It is not adaptive to the statistics of the training corpus."
    },
    "answer": "C",
    "explaination": "time 1:10:04–1:10:39"
  },
  {
    "week": 1,
    "problem_numer": 14,
    "type": "multiple-choice",
    "question": "What is the fundamental principle of the BPE algorithm?",
    "options": {
      "A": "It splits words based on a fixed set of grammatical rules.",
      "B": "It learns to merge the most frequently occurring adjacent pair of tokens iteratively.",
      "C": "It assigns a unique token to every word found in the training corpus.",
      "D": "It breaks down all text into individual bytes."
    },
    "answer": "B",
    "explaination": "time 1:12:49–1:12:54"
  },
  {
    "week": 1,
    "problem_numer": 15,
    "type": "multiple-choice",
    "question": "When using a trained BPE tokenizer to encode a new string, what is the process?",
    "options": {
      "A": "The string is first broken into words, and each word is looked up in the vocabulary.",
      "B": "The string is converted to bytes, and then the learned merge operations are replayed in the same order they were learned.",
      "C": "The string is compared against the entire training corpus to find the closest match.",
      "D": "The BPE algorithm is re-run from scratch on the new string."
    },
    "answer": "B",
    "explaination": "time 1:16:44–1:16:51"
  },
  {
    "week": 1,
    "problem_numer": 16,
    "type": "multiple-choice",
    "question": "The lecture mentions a \"compression ratio\" for tokenizers. What does this ratio represent?",
    "options": {
      "A": "The number of tokens divided by the number of words.",
      "B": "The number of bytes in the original string divided by the number of tokens generated.",
      "C": "The size of the model vocabulary divided by the size of the training data.",
      "D": "The time it takes to encode versus decode a string."
    },
    "answer": "B",
    "explaination": "time 1:04:40–1:04:48"
  },
  {
    "week": 1,
    "problem_numer": 17,
    "type": "multiple-choice",
    "question": "The lecture mentions that if you have a lot of data but are compute-constrained, you might filter data aggressively. Why?",
    "options": {
      "A": "To reduce data storage costs.",
      "B": "To avoid legal issues with copyrighted data.",
      "C": "To avoid wasting precious compute on bad or irrelevant data.",
      "D": "To make the model smaller and faster for inference."
    },
    "answer": "C",
    "explaination": "time 56:43–56:49"
  },
  {
    "week": 1,
    "problem_numer": 18,
    "type": "multiple-choice",
    "question": "What distinguishes the \"pre-fill\" phase from the \"decode\" phase in inference?",
    "options": {
      "A": "Pre-fill is memory-bound, while decode is compute-bound.",
      "B": "Pre-fill processes the prompt tokens in parallel, while decode generates tokens one by one.",
      "C": "Pre-fill happens during training, while decode happens during deployment.",
      "D": "Pre-fill uses a small model, while decode uses a large model."
    },
    "answer": "B",
    "explaination": "time 38:29–38:41"
  },
  {
    "week": 1,
    "problem_numer": 19,
    "type": "multiple-choice",
    "question": "Why is having a good base model crucial before the alignment phase?",
    "options": {
      "A": "Alignment cannot add new knowledge; it can only modify a model that already has raw potential.",
      "B": "A good base model is smaller and cheaper to align.",
      "C": "The alignment algorithms like DPO only work on models trained on web-scale data.",
      "D": "The base model determines the final vocabulary size."
    },
    "answer": "A",
    "explaination": "time 50:36–52:12"
  },
  {
    "week": 1,
    "problem_numer": 20,
    "type": "multiple-choice",
    "question": "By convention, where is the space character typically placed by tokenizers like BPE?",
    "options": {
      "A": "It is discarded during tokenization.",
      "B": "It is appended to the end of a token.",
      "C": "It is converted into a special <SPACE> token.",
      "D": "It is prepended to the beginning of the following token."
    },
    "answer": "D",
    "explaination": "time 1:02:19–1:02:19"
  }
]
